from collections import defaultdict
from typing import Optional

import numpy as np
import torch
from tensordict import TensorDict
from torchrl.data import UnboundedContinuousTensorSpec, DiscreteTensorSpec
from torchrl.envs import EnvBase
from torchrl.envs.utils import check_env_specs
import gymnasium as gym
import soulsgym


def flatten_observation(obs):
    """
    Convert the custom observation dictionary into a 1D torch tensor.
    """
    # Flatten scalar values and convert to float
    scalar_values = torch.tensor([
        obs['phase'],
        obs['player_hp'][0],
        obs['player_max_hp'],
        obs['player_sp'][0],
        obs['player_max_sp'],
        obs['boss_hp'][0],
        obs['boss_max_hp'],
        obs['player_animation'],
        obs['player_animation_duration'][0],
        obs['boss_animation'],
        obs['boss_animation_duration'][0],
        int(obs['lock_on'])  # Convert boolean to int
    ], dtype=torch.float32)

    # Flatten arrays and convert to torch tensors
    pose_values = torch.tensor(
        np.concatenate([
            obs['player_pose'],
            obs['boss_pose'],
            obs['camera_pose']
        ]),
        dtype=torch.float32
    )

    # Concatenate scalar and pose values
    flat_obs = torch.cat([scalar_values, pose_values])

    return flat_obs


class TorchRLSoulsGymWrapper(EnvBase):
    def __init__(self, env_name="SoulsGymIudex-v0", device="cpu"):
        super().__init__(device=device)
        self.env = gym.make(env_name)
        obs_shape = self._get_flattened_obs(self.env.reset()).shape
        self.observation_spec = UnboundedContinuousTensorSpec(shape=obs_shape, dtype=torch.float32)
        self.action_spec = DiscreteTensorSpec(self.env.action_space.n)
        # Update reward_spec and done_spec to have a non-empty shape
        self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,), dtype=torch.float32)
        self.done_spec = DiscreteTensorSpec(2, shape=(1,), dtype=torch.bool)

    def _get_flattened_obs(self, obs):
        if isinstance(obs, tuple):  # Unpack (obs_dict, info_dict)
            obs = obs[0]
        return flatten_observation(obs)

    def _reset(self, tensordict=None):
        obs = self.env.reset()
        obs_tensor = self._get_flattened_obs(obs).to(self.device)
        return TensorDict({
            "observation": obs_tensor
        }, batch_size=[])

    def _step(self, tensordict):
        action = tensordict["action"].item()
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # Log the reward at the step level
        print(f"Reward received at step: {reward}")
        
        # Ensure reward is valid
        if reward is None:
            print("Warning: Reward is None, defaulting to 0.")
            reward = 0
        else:
            reward = float(reward)
        
        # Check reward shape before passing it into the TensorDict
        reward_tensor = torch.tensor(reward, dtype=torch.float32).view(1)
        print(f"Reward tensor (pre-TensorDict) shape: {reward_tensor.shape}")
        
        done = terminated or truncated
        obs_tensor = self._get_flattened_obs(obs).to(self.device)

        # Log the TensorDict creation process
        print(f"Creating TensorDict with action and reward.")
        
        return TensorDict({
            "action": torch.tensor(action),
            "next": {
                "observation": obs_tensor,
                "reward": reward_tensor,
                "done": torch.tensor(done, dtype=torch.bool)
            }
        }, batch_size=[])


    def _set_seed(self, seed):
        self.env.seed(seed)


# Initialize and check specs
env = TorchRLSoulsGymWrapper()
check_env_specs(env)

td = env.reset()
print("Observation TensorDict:")
print(td)

print("\nObservation spec shape:", env.observation_spec.shape)
print("Action spec:", env.action_spec)
